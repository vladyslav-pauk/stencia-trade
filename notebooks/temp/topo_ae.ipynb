{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from frameworks.data.time_series import generate_time_series\n",
    "from frameworks.tda.embedding import embedding_time_series\n",
    "from frameworks.utils.plots import plot_run_chart, plot_persistence_diagram\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import Amplitude\n",
    "\n",
    "figsize = 4\n",
    "\n",
    "signal_types = [\"random\", \"periodic\", \"quasi-periodic\", \"oscillatory\", \"log-periodic-power\", \"geometric_random_walk\", \"random_walk_critical\"]\n",
    "length = 15\n",
    "embedding_delay = 1\n",
    "embedding_dimension = 2\n",
    "sliding_window_size = 10\n",
    "sliding_stride = 1\n",
    "\n",
    "signals = []\n",
    "fig, axes = plt.subplots(1, len(signal_types), figsize=(figsize * len(signal_types), figsize))\n",
    "for i, signal_type in enumerate(signal_types):\n",
    "    t, signal = generate_time_series(\n",
    "        length=length,\n",
    "        signal_type=signal_type,\n",
    "        snr=40,\n",
    "        amplitude=1,\n",
    "        frequency=10,\n",
    "        amplitude_ratio=0.25,\n",
    "        frequency_ratio=0.3,\n",
    "        alpha=0.5,\n",
    "        exponential_factor=-1,\n",
    "        nonlinearity=0.5,\n",
    "        critical_time=0.75\n",
    "    )\n",
    "    signals.append(signal)\n",
    "    plot_run_chart(axes[i], t, signal, label=\"Signal\")\n",
    "\n",
    "embedded_signals = [embedding_time_series(signal, embedding_delay, embedding_dimension, sliding_window_size, stride=sliding_stride) for signal in signals]\n",
    "\n",
    "\n",
    "class TopoAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(TopoAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "\n",
    "# class TopoAutoencoder(nn.Module):\n",
    "#     def __init__(self, input_dim, latent_dim):\n",
    "#         super(TopoAutoencoder, self).__init__()\n",
    "#\n",
    "#         # Encoder using LSTM\n",
    "#         self.encoder = nn.LSTM(input_dim, latent_dim, batch_first=True)\n",
    "#\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.LSTM(latent_dim, input_dim, batch_first=True)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         _, (z, _) = self.encoder(x)  # Get last hidden state\n",
    "#         x_recon, _ = self.decoder(z.unsqueeze(0))  # Decode latent state\n",
    "#         return x_recon, z.squeeze(0)\n",
    "\n",
    "\n",
    "def compute_persistence_diagram(data):\n",
    "    \"\"\"Compute persistence diagram using Vietoris-Rips complex.\"\"\"\n",
    "    diagrams = []\n",
    "    for signal in data:  # Iterate over batch\n",
    "        signal = signal.reshape(-1, 1)  # Reshape each individual sequence\n",
    "\n",
    "        vr = VietorisRipsPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "        diagrams.append(vr.fit_transform([signal]))  # Pass as a list\n",
    "    return diagrams\n",
    "\n",
    "\n",
    "def topological_loss(diagrams_x, diagrams_z):\n",
    "    \"\"\"Compute topological loss as the difference in persistence diagrams.\"\"\"\n",
    "    amplitude = Amplitude(metric=\"wasserstein\", metric_params={\"p\": 1})\n",
    "    loss_x = amplitude.fit_transform(diagrams_x)\n",
    "    loss_z = amplitude.fit_transform(diagrams_z)\n",
    "    return torch.tensor(np.mean(np.abs(loss_x - loss_z)), requires_grad=True)\n",
    "\n",
    "\n",
    "# Training Setup\n",
    "latent_dim = 2  # Low-dimensional space\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TopoAutoencoder(input_dim=embedding_dimension, latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "# Convert embedded_signals to a batch tensor (3D: batch_size, seq_length, embedding_dim)\n",
    "x = torch.stack([torch.tensor(signal[0], dtype=torch.float32) for signal in embedded_signals]).to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    x_recon, z = model(x)\n",
    "\n",
    "    # Compute persistence diagrams for each sample in batch\n",
    "    diagrams_x = compute_persistence_diagram(x.cpu().detach().numpy())[0]\n",
    "    diagrams_z = compute_persistence_diagram(z.cpu().detach().numpy())[0]\n",
    "\n",
    "    # Compute losses\n",
    "    loss_recon = mse_loss(x_recon, x)\n",
    "    loss_topo = topological_loss(diagrams_x, diagrams_z)\n",
    "    loss = loss_recon + 0.1 * loss_topo  # Weighted combination\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from persim import plot_diagrams\n",
    "\n",
    "# Select a sample signal\n",
    "signal_idx = 5\n",
    "signal = embedded_signals[signal_idx]\n",
    "\n",
    "# Compute Persistence Diagrams\n",
    "diagrams_x = compute_persistence_diagram(signal)\n",
    "diagrams_z = compute_persistence_diagram(\n",
    "    model.encoder(torch.tensor(signal[0], dtype=torch.float32).to(device)).cpu().detach().numpy())\n",
    "\n",
    "z = model.encoder(torch.tensor(signal[0], dtype=torch.float32).to(device)).clone().detach()\n",
    "x_rec = model.decoder(z).cpu().detach()\n",
    "diagrams_x_rec = compute_persistence_diagram(x_rec.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plot_persistence_diagram(diagrams_x[0][0], ax=axes[0])\n",
    "axes[0].set_title(\"Persistence Diagram - Input Space\")\n",
    "\n",
    "plot_persistence_diagram(diagrams_z[0][0], ax=axes[1])\n",
    "axes[1].set_title(\"Persistence Diagram - Latent Space\")\n",
    "# plt.show()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "# mse_losses = []\n",
    "# topo_losses = []\n",
    "#\n",
    "# for signal in embedded_signals:\n",
    "#     x = torch.tensor(signal[0], dtype=torch.float32).to(device)\n",
    "#     x_recon, z = model(x)\n",
    "#\n",
    "#     # Compute persistence diagrams\n",
    "#     diagrams_x = compute_persistence_diagram(x.cpu().detach().numpy())[0]\n",
    "#     diagrams_z = compute_persistence_diagram(z.cpu().detach().numpy())[0]\n",
    "#\n",
    "#     # Compute losses\n",
    "#     mse_losses.append(mse_loss(x_recon, x).item())\n",
    "#     topo_losses.append(topological_loss(diagrams_x, diagrams_z).item())\n",
    "#\n",
    "# # Print Table\n",
    "# import pandas as pd\n",
    "# df_results = pd.DataFrame({\"Signal Type\": signal_types, \"MSE Loss\": mse_losses, \"Topological Loss\": topo_losses})\n",
    "# print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
